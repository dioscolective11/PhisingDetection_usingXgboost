# -*- coding: utf-8 -*-
"""PhisingDetection_usingXgboost.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cPehFIruFNmiCNDaZwtUZUzbLluLV8G7
"""

import pandas as pd
import numpy as np
import itertools
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from lightgbm import LGBMClassifier

# Load the dataset
df = pd.read_csv('/content/malicious_phish.csv')

# Display basic information about the dataset
print("Dataset Info:")
print(df.info())
print("\nFirst 5 rows of the dataset:")
print(df.head())
print("\nLabel Distribution:")
print(df['type'].value_counts())

# Feature extraction function
def extract_features(url):
    features = {}
    features['length'] = len(url)
    features['num_digits'] = sum(c.isdigit() for c in url)
    features['num_special'] = sum(not c.isalnum() for c in url)
    features['num_subdomains'] = url.count('.') - 1  # Simple subdomain count
    features['has_https'] = int(url.startswith('https://'))
    features['has_http'] = int(url.startswith('http://'))
    return pd.Series(features)

# Apply feature extraction
features = df['url'].apply(extract_features)
X = features
y = df['type'].apply(lambda x: 1 if x == 'phishing' else 0)  # Binary classification: phishing (1) or not (0)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')
lgbm_model = LGBMClassifier()
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train models
xgb_model.fit(X_train, y_train)
lgbm_model.fit(X_train, y_train)
rf_model.fit(X_train, y_train)

# Make predictions
xgb_predictions = xgb_model.predict(X_test)
lgbm_predictions = lgbm_model.predict(X_test)
rf_predictions = rf_model.predict(X_test)

# Evaluate models
def evaluate_model(y_true, y_pred, model_name):
    print(f"Evaluation for {model_name}:")
    print(classification_report(y_true, y_pred))
    print("Confusion Matrix:")
    cm = confusion_matrix(y_true, y_pred)
    print(cm)
    accuracy = accuracy_score(y_true, y_pred)
    print(f"Accuracy: {accuracy:.2f}")
    return cm

# Evaluate XGBoost model
xgb_cm = evaluate_model(y_test, xgb_predictions, "XGBoost")

# Evaluate LightGBM model
lgbm_cm = evaluate_model(y_test, lgbm_predictions, "LightGBM")

# Evaluate Random Forest model
rf_cm = evaluate_model(y_test, rf_predictions, "Random Forest")

# Plot confusion matrices
def plot_confusion_matrix(cm, model_name):
    plt.figure(figsize=(6, 4))
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title(f'Confusion Matrix for {model_name}')
    plt.colorbar()
    tick_marks = np.arange(2)
    plt.xticks(tick_marks, ['Legitimate', 'Phishing'])
    plt.yticks(tick_marks, ['Legitimate', 'Phishing'])

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()

# Plot confusion matrices for all models
plot_confusion_matrix(xgb_cm, "XGBoost")
plt.show()

plot_confusion_matrix(lgbm_cm, "LightGBM")
plt.show()

plot_confusion_matrix(rf_cm, "Random Forest")
plt.show()

# Feature importance for Random Forest
plt.figure(figsize=(10, 6))
importances = rf_model.feature_importances_
indices = np.argsort(importances)[::-1]
plt.title('Feature Importances for Random Forest')
plt.bar(range(X.shape[1]), importances[indices], align='center')
plt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.tight_layout()
plt.show()

# Feature importance for XGBoost
plt.figure(figsize=(10, 6))
xgb_importances = xgb_model.feature_importances_
indices = np.argsort(xgb_importances)[::-1]
plt.title('Feature Importances for XGBoost')
plt.bar(range(X.shape[1]), xgb_importances[indices], align='center')
plt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.tight_layout()
plt.show()

# Feature importance for LightGBM
plt.figure(figsize=(10, 6))
lgbm_importances = lgbm_model.feature_importances_
indices = np.argsort(lgbm_importances)[::-1]
plt.title('Feature Importances for LightGBM')
plt.bar(range(X.shape[1]), lgbm_importances[indices], align='center')
plt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.tight_layout()
plt.show()

# Compare model accuracies
model_names = ['XGBoost', 'LightGBM', 'Random Forest']
accuracies = [
    accuracy_score(y_test, xgb_predictions),
    accuracy_score(y_test, lgbm_predictions),
    accuracy_score(y_test, rf_predictions)
]

plt.figure(figsize=(8, 6))
plt.bar(model_names, accuracies, color=['blue', 'green', 'red'])
plt.title('Model Accuracy Comparison')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
plt.show()

# Print final summary
print("\nFinal Summary of Model Accuracies:")
for model, acc in zip(model_names, accuracies):
    print(f"{model}: {acc:.4f}")



"""# New Section"""